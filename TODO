# This file is automatically generated by generate_todo.py.
# Files that start with an underscore ("_") have been excluded.

./parsimony/estimators.py:
-------------------------
87: # TODO: Is this a good name?

282: # TODO: Should we use a seed here so that we get deterministic results?

394: # TODO: Should we use a seed somewhere so that we get deterministic results?

509: # TODO: Should we use a seed here so that we get deterministic results?

629: # TODO: Should we use a seed here so that we get deterministic results?

789: # TODO: Should we use a seed here so that we get deterministic results?

954: # TODO: Should we use a seed here so that we get deterministic results?

1206: # TODO: Should we use a seed here so that we get deterministic results?

1327: # TODO: Should we use a seed here so that we get deterministic results?

1495: # TODO: Should we use a seed here so that we get deterministic results?

1671: # TODO: Should we use a seed here so that we get deterministic results?

1933: # TODO: Should we use a seed here so that we get deterministic
1934: # results?
1935: #                if w is None or k > 0:

2152: # TODO: Should we use a seed here so that we get deterministic
2153: # results?
2154: #                if w is None or k > 0:

./parsimony/utils/plot.py:
-------------------------
72: # TODO: Add the other cases.

./parsimony/utils/utils.py:
--------------------------
24: #TODO: This depends on the OS. We should try to be clever here ...

./parsimony/utils/consts.py:
---------------------------
17: # TODO: MAX_ITER is heavily algorithm-dependent, so we have to think about if
18: # we should include a package-wide maximum at all.

./parsimony/functions/losses.py:
-------------------------------
187: # TODO: Inherit from LinearRegression and add an L2 constraint instead!

364: # TODO: Make the weights sparse.
365: #weights = np.eye(self.X.shape[0])

367: # TODO: Allow the weight vector to be a list.

472: # TODO: Use FastSVD for speedup!

474: self._L = np.max(s) ** 2.0  # TODO: CHECK

635: PWX = 0.5 * np.sqrt(self.weights) * self.X  # TODO: CHECK WITH FOUAD
636: # PW = 0.5 * np.eye(self.X.shape[0]) ## miss np.sqrt(self.W)
637: #PW = 0.5 * np.sqrt(self.W)
638: #PWX = np.dot(PW, self.X)
639: # TODO: Use FastSVD for speedup!

641: self._L = np.max(s) ** 2.0  # TODO: CHECK

646: self._L += self.k  # TODO: CHECK

664: # TODO: Handle mean here?

./parsimony/functions/penalties.py:
----------------------------------
198: # TODO: This should not be able to happen! Do we know it doesn't?

203: # TODO: This should not be able to happen! Do we know it doesn't?

571: # TODO: Check if this is correct!

1340: # TODO: We can share variables between f and df and speed up
1341: # some shared computations.

./parsimony/functions/properties.py:
-----------------------------------
109: # TODO: Should all constraints have the projection operator?

355: # TODO: Should L by default take a weight vector as argument?

717: # TODO: This only work if the elements of self._A are scipy.sparse. We
718: # should allow dense matrices as well.

724: # TODO: Add max_iter here!

792: # TODO: Avoid stacking here.

794: # TODO: Add max_iter here!

845: # TODO: Compute with FISTA instead!

./parsimony/functions/combinedfunctions.py:
------------------------------------------
37: # TODO: Add penalty_start and mean to all of these!

109: # TODO: We currently only allow one proximal operator. Fix this!

162: # TODO: We currently only allow one proximal operator. Fix this!

656: # TODO: This is not good. Solve this better!

1158: # TODO: This is not a good solution. Can we solve this in a better way?

1210: # TODO: Use max_iter here!!

1260: # TODO: Kernelise this function! See how I did in
1261: # LinearRegressionL1L2TV._beta_hat.

1303: # TODO: Add this function or refactor API!

1386: # TODO: This is not a nice solution. Can we solve it better?

./parsimony/functions/multiblock/losses.py:
------------------------------------------
899: # TODO: Check instead if it is a numpy array.

./parsimony/functions/nesterov/tv.py:
------------------------------------
151: # TODO: This only work if the elements of self._A are scipy.sparse. We
152: # should allow dense matrices as well.

155: # TODO: Instead of p, this should really be the number of non-zero
156: # rows of A.

166: # TODO: Add max_iter here!

./parsimony/functions/nesterov/l1tv.py:
--------------------------------------
62: # WARNING: Number of non-zero rows may differ from p.

145: # TODO: Instead of p, this should really be the number of non-zero
146: # rows of A.

157: # TODO: Add max_iter here!!

266: # TODO: Do we need to take the number of variables here?
267: # Why not use np.prod(shape) + penalty_start instead and save a parameter?

290: # TODO: Do we need to take the number of variables here?
291: # Why not use np.prod(shape) + penalty_start instead and save a parameter?

./parsimony/functions/nesterov/grouptv.py:
-----------------------------------------
158: # TODO: This only work if the elements of self._A are scipy.sparse. We
159: # should allow dense matrices as well.

165: # TODO: Add max_iter here!

./parsimony/datasets/regression/dice5.py:
----------------------------------------
35: # TODO: This is wrong. Shape should be Z, Y, X.

./parsimony/algorithms/nipals.py:
--------------------------------
32: # TODO: Add information about the run.

./parsimony/algorithms/primaldual.py:
------------------------------------
202: # TODO: Warn if G_new < 0.

./parsimony/algorithms/utils.py:
-------------------------------
34: # TODO: This class should be replaced with Enum.

265: # TODO: We already have f_mid, so we can return a better approximation
266: # here!

356: # TODO: Handle the other cases!

368: # TODO: We seek a root, i.e. where f(x) = 0. The stopping criterion
369: #       should (could?) thus be abs(f(x)) <= eps!

388: if abs(x - x_) <= self.eps:  # TODO: Stopping criterion. See above!

./parsimony/algorithms/bases.py:
-------------------------------
57: # TODO: Replace the one in BaseAlgorithm.

./parsimony/algorithms/proximal.py:
----------------------------------
328: else:  # TODO: Fix this!

404: # TODO: Investigate what is a good default value here!

458: # TODO: Investigate what is a good default value here!

511: # TODO: Investigate what is a good default value here!

550: # TODO: Does the weights really matter when the function is the
551: # indicator function?

585: # TODO: Investigate what is a good default value here!

